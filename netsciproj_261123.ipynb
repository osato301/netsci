{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d584e-6b39-4699-9e0f-fd6eef39313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making network info usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26aa8cdf-0d8f-44fb-9fb5-101c5eb33cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import networkx as nx\n",
    "import base64\n",
    "import requests\n",
    "# import spotipy\n",
    "# from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d681b2b-3609-4222-b71c-8093218a0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload original csv\n",
    "cover_df = pd.read_csv('/Users/oliviasato/Desktop/Network Science/covers_with_artists.csv')\n",
    "#get rid of nans\n",
    "cover_df = cover_df.fillna('')\n",
    "\n",
    "#put song titles in same rows as artists\n",
    "for i in range(0,len(cover_df)):\n",
    "    if cover_df['Song'][i] == '':\n",
    "        cover_df['Song'][i] = cover_df['Song'][i-1]\n",
    "    #remove negative/lowercase problematic tracks\n",
    "    if cover_df['Song'][i][0] == '-':\n",
    "        cover_df['Track ID'][i] = ''\n",
    "columns = ['Song', 'Track ID', 'Artist ID', 'Artist Name']\n",
    "no_empties = pd.DataFrame(columns=columns)\n",
    "for i in range(0,len(cover_df)):\n",
    "    if not cover_df['Track ID'][i] == '':\n",
    "        row_to_add = pd.DataFrame(cover_df.iloc[i]).T\n",
    "        no_empties = pd.concat([no_empties, row_to_add], ignore_index=True)\n",
    "cover_df = no_empties\n",
    "\n",
    "#make 2 node lists, one of unique songs and another of unique artists\n",
    "artists = []\n",
    "songs = []\n",
    "for i in range(len(cover_df)):  \n",
    "    if not (cover_df['Artist Name'][i] in artists):\n",
    "        artists.append(cover_df['Artist Name'][i])\n",
    "    if not (cover_df['Song'][i] in songs):\n",
    "        songs.append(cover_df['Song'][i])\n",
    "\n",
    "#seperating song IDs and adding to seperate column\n",
    "song_id = []\n",
    "songs_cleaned = []\n",
    "for i in range(0,len(cover_df)):\n",
    "    txt = cover_df['Song'][i]\n",
    "    split = re.split(', ', txt)\n",
    "    songid = split[0]\n",
    "    title = split[-1]\n",
    "    song_id.append(songid)\n",
    "    songs_cleaned.append(title)\n",
    "songs = songs_cleaned\n",
    "\n",
    "for i in range(0,len(cover_df)):\n",
    "    cover_df['Song'][i] = songs[i]\n",
    "\n",
    "cover_df['Song ID'] = song_id\n",
    "cover_df['Original Artist'] = len(cover_df)*[()]\n",
    "cover_df['Original Year'] = len(cover_df)*[()]\n",
    "#eventually add year of cover here\n",
    "cover_df = cover_df[['Song ID', 'Song', 'Original Artist', 'Original Year', 'Track ID', 'Artist ID', 'Artist Name']]\n",
    "\n",
    "cover_df.to_csv('cover_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "765ffbca-7e2f-44d3-b028-aef689f3b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further modifying df\n",
    "cover_df = pd.read_csv('cover_data.csv')\n",
    "\n",
    "#take only second song id when there are two\n",
    "id2 = []\n",
    "for i in range(0,len(cover_df)):\n",
    "    txt = cover_df['Song ID'][i]\n",
    "    split = re.split(',', txt)\n",
    "    second = split[-1]\n",
    "    id2.append(second)\n",
    "\n",
    "for i in range(0,len(cover_df)):\n",
    "    cover_df['Song ID'][i] = id2[i]\n",
    "cover_df.to_csv('cover_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81b3cb-e59a-4de8-85bf-ef67ebf9c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCRAPING STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10ebb71b-6725-45f0-b6a3-078075f00627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping info\n",
    "#these are personal to my spotify developer account. Feel free to use but you can also make your own very easily w/ https://developer.spotify.com/documentation/web-api\n",
    "client_id = '98fa7d752c1d472da28147c9ebd3b277'\n",
    "client_secret = '98a66db9861d4939aa2036edeabd924a'\n",
    "\n",
    "# Encode client ID and client secret\n",
    "encoded = base64.b64encode(f\"{client_id}:{client_secret}\".encode()).decode()\n",
    "\n",
    "# Set up headers for authentication\n",
    "headers = {'Authorization': f'Basic {encoded}',}\n",
    "\n",
    "# Set up data for token request\n",
    "data = {'grant_type': 'client_credentials',}\n",
    "\n",
    "# Make the token request\n",
    "response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
    "\n",
    "# Get the access token from the response\n",
    "access_token = response.json().get('access_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "545b1684-f1b4-4e3a-b215-315d2b2db396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding empty columns to be scraped\n",
    "cover_df = pd.read_csv('cover_data.csv')\n",
    "cover_df['Cover Genre'] = ''\n",
    "cover_df['Popularity'] = None\n",
    "cover_df['Cover Date'] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46a6f3fc-054f-4d99-84ff-cb72c5e9342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_df.to_csv('cover_data_artistinfoscraped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a0bf6-22b5-47e7-bed9-9c1f4563b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded here because I scraped in increments, so I kept rerunning this to check how the csv was updating\n",
    "cover_df = pd.read_csv('cover_data_artistinfoscraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacd125-3154-43c5-b8a1-b99d1e3ecdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time is IMPORTANT\n",
    "#I got booted for a couple of hours when I skipped it. I believe limit is 200 queries a minute, which is why I broke this into 200 cover increments\n",
    "import time\n",
    "#upper limit actually above 80*200 (its the length of cover_df) but I did these last ones outside the loop\n",
    "for j in range(0,80):\n",
    "    for i in range(j*200,(j+1)*200):\n",
    "        cover_artist = cover_df['Artist Name'][i]\n",
    "        search_url = f\"https://api.spotify.com/v1/search\"\n",
    "        search_params = {\"q\": cover_artist, \"type\": \"artist\"}\n",
    "        search_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "        search_response = requests.get(search_url, params=search_params, headers=search_headers)\n",
    "        search_results = search_response.json()\n",
    "        if \"artists\" in search_results and \"items\" in search_results[\"artists\"]:\n",
    "            artists = search_results[\"artists\"][\"items\"]\n",
    "        if artists:\n",
    "            #assume relevant artist is first to pop up\n",
    "            name = artists[0]\n",
    "        cover_df['Cover Genre'][i] = name[\"genres\"]\n",
    "        cover_df['Popularity'][i] = name[\"popularity\"]\n",
    "    time.sleep(10)\n",
    "cover_df.to_csv('cover_data_artistinfoscraped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49919f-4bea-4427-a28c-003ab2dea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping original artist and date from Spotify\n",
    "og_artist_list = []\n",
    "og_date_list = []\n",
    "\n",
    "for j in range(0,80):\n",
    "    for i in range(j*200,(j+1)*200):\n",
    "        song_raw = cover_df['Song'][i]\n",
    "        split = re.split(r' \\(', song_raw)\n",
    "        song_name = split[0]\n",
    "        search_url = f\"https://api.spotify.com/v1/search\"\n",
    "        search_params = {\"q\": song_name, \"type\": \"track\"}\n",
    "        search_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "        search_response = requests.get(search_url, params=search_params, headers=search_headers)\n",
    "        search_results = search_response.json()\n",
    "        \n",
    "        columns = ['Artists','Date']\n",
    "        artists_dates_df = pd.DataFrame(columns=columns)\n",
    "        \n",
    "        #only collect og info for first instance of song title\n",
    "        if i>0 and song_raw == cover_df['Song'][i-1]:\n",
    "            #to be filled later, just not wasting time\n",
    "            og_date = ''\n",
    "            og_artist = ''\n",
    "        \n",
    "        #if the query yields results, make df of all results and select the one with the earliest release date\n",
    "        elif \"tracks\" in search_results and \"items\" in search_results[\"tracks\"]:\n",
    "            tracks = search_results[\"tracks\"][\"items\"]\n",
    "            for k in tracks:\n",
    "                #spotify sometimes returns entirely different tracks from those I am searching? Weird algorithm behavior\n",
    "                #I think its suggesting what it thinks we want, filter that out\n",
    "                if song_name in k['name']:\n",
    "                    artists = [artist[\"name\"] for artist in k[\"artists\"]]\n",
    "                    release_date = k[\"album\"][\"release_date\"]\n",
    "                    track_df = pd.DataFrame({'Artists': [artists], 'Date': [release_date]})\n",
    "                    artists_dates_df = pd.concat([artists_dates_df, track_df], ignore_index=True)  \n",
    "            if artists_dates_df.empty == False:\n",
    "                og_date = min(artists_dates_df['Date'])\n",
    "                row_index = (artists_dates_df == og_date).any(axis=1).idxmax()\n",
    "                og_artist = artists_dates_df['Artists'][row_index]\n",
    "            \n",
    "        else:\n",
    "            print(\"Not in search results.\")\n",
    "            og_date = ''\n",
    "            og_artist = ''\n",
    "\n",
    "        og_artist_list.append(og_artist)\n",
    "        og_date_list.append(og_date)\n",
    "\n",
    "#MIND THE LIMITS when breaking into smaller bits, modify i+() term so that it matches lower j limit above\n",
    "for i in range(0,len(og_artist_list)):\n",
    "    cover_df['Original Artist'][i+0*200] = og_artist_list[i]\n",
    "    cover_df['Original Year'][i+0*200] = og_date_list[i]\n",
    "    \n",
    "#FIX LATER: once I scrape cover date, filter by those whose cover date precedes og date and redo og date and og artist\n",
    "\n",
    "cover_df.to_csv('with_ogs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5105a38-2fa8-41f4-ad06-8eb51a263d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encountering problem where good tracks are not meeting requirements to retrieve release_date, so use fuzzy matching\n",
    "def is_fuzzy_in_list(test, string_list, threshold=80):\n",
    "    for i in string_list:\n",
    "        similarity = fuzz.partial_ratio(test, i)\n",
    "        if similarity >= threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c018f1d6-56ed-4454-8d28-afb1fa829657",
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_df = pd.read_csv('with_coverdates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed2206de-b682-4afa-b1f1-7af831dd9ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/rvt7flcd7qzcbq_87lq3c8rr0000gn/T/ipykernel_52984/631394392.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cover_df['Cover Date'][i+69*200] = cover_date[i]\n"
     ]
    }
   ],
   "source": [
    "#get cover date\n",
    "#again, choosing earliest cover date because I am seeing problems with remastered versions etc. spitting out covers after the artists died\n",
    "\n",
    "import time\n",
    "\n",
    "cover_date = []\n",
    "\n",
    "for j in range(0,80):\n",
    "    for i in range(j*200,(j+1)*200):\n",
    "        song_raw = cover_df['Song'][i]\n",
    "        split = re.split(r' \\(', song_raw)\n",
    "        song_name = split[0]\n",
    "        cover_artist = cover_df['Artist Name'][i]\n",
    "        query = f'artist:{cover_artist} track:{song_name}'\n",
    "        search_url = f\"https://api.spotify.com/v1/search\"\n",
    "        search_params = {\"q\": query, \"type\": \"track\"}\n",
    "        search_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "        search_response = requests.get(search_url, params=search_params, headers=search_headers)\n",
    "        search_results = search_response.json()\n",
    "                 \n",
    "        potential_dates = []\n",
    "        \n",
    "        if \"tracks\" in search_results and \"items\" in search_results[\"tracks\"]:\n",
    "            tracks = search_results[\"tracks\"][\"items\"]\n",
    "            if len(tracks) == 0:\n",
    "                release_date = ''\n",
    "            else:\n",
    "                for k in tracks:\n",
    "                    if song_name in k[\"name\"]:\n",
    "                        artists = [artist[\"name\"] for artist in k[\"artists\"]]\n",
    "                        if is_fuzzy_in_list(cover_artist,artists):\n",
    "                            potential_dates.append(k[\"album\"][\"release_date\"])\n",
    "                if len(potential_dates) > 0:\n",
    "                    release_date = min(potential_dates)\n",
    "                else:\n",
    "                    release_date = ''\n",
    "            \n",
    "        else:\n",
    "            release_date = ''\n",
    "            \n",
    "        cover_date.append(release_date)\n",
    "        \n",
    "    time.sleep(20)\n",
    "\n",
    "#MIND THE LIMITS\n",
    "for i in range(0,len(cover_date)):\n",
    "    cover_df['Cover Date'][i+0*200] = cover_date[i]\n",
    "\n",
    "cover_df.to_csv('with_coverdates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd262a8a-411e-4166-a2c8-4680755bb488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuzz = 0.6714966871013686\n"
     ]
    }
   ],
   "source": [
    "#number of cover years with dates using fuzzy matching\n",
    "fuzz_df = pd.read_csv('with_coverdates.csv')\n",
    "counter_fuzz = 0\n",
    "for i in range(0,len(fuzz_df)):\n",
    "    if type(fuzz_df['Cover Date'][i]) == str:\n",
    "        counter_fuzz += 1\n",
    "print('fuzz = ' + str(counter_fuzz/16149))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac0d9adf-7f6b-4e27-b255-81b78ea5083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#three rounds of scraping\n",
    "#should I split artists if multiple names?\n",
    "\n",
    "#FIRST - ARTIST NODES LIST: cover artist genre, cover artist popularity - DONE\n",
    "\n",
    "#SECOND - SONG NODES LIST: og artist, og year of release - DONE\n",
    "    #perhaps have to double check on secondhandsongs?\n",
    "\n",
    "#THIRD - COVER LINKS (probs from df): year of cover - DONE (first pass at 67%)\n",
    "\n",
    "#FOURTH - CHECK (og date < cover date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76af12-75bc-459f-9802-bc6a3fbd7a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
